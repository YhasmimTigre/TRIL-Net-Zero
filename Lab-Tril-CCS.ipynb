{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab. TRIL - Carbon Capture and Storage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Engenharia Computacional para a Emissão Zero no Setor de Óleo e Gás"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse é um programa que realiza um processo de leitura, processamento e análise de um conjunto de dados de um reservatório, para encontrar classes de injetividade apresentadas com análise de gráfico.\n",
    "\n",
    "Esse projeto é desenvolvido por:<br>\n",
    "* Yhasmim de Souza Tigre - Aluna de Iniciação Científica - UFPB\n",
    "* Prof. Dr. Gustavo C. Oliveira - Professor Orientador - UFPB\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from numba import jit \n",
    "#%matplotlib inline \n",
    "#from matplotlib.pyplot import plotpip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "#Serve para ignorar os 'red warnings' que algumas bibliotecas apontam porque tem novas versoes de implementacao\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Preparação de Files "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essas funções devem receber arquivos csv (em uma formatação pré-estabelecida) que serem lidos, checados e iniciados.<br>\n",
    "Os arquivos precisam:\n",
    "* ter colunas nomeadas (de preferência em ordem alfabética)\n",
    "* não ter valores NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''função de leitura do arquivo csv para df'''\n",
    "\n",
    "def read_df(df):\n",
    "    #limpeza (valores NAN)\n",
    "    if any(df.isna()):\n",
    "        print(\"df com valores NAN\")\n",
    "        #for i in df: #testar sem for\n",
    "        df = df.fillna(0)\n",
    "        print(\"df corrigido, não mais possui valores NAN\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''função de checagem de keys em dataframe '''\n",
    "\n",
    "def checking(df):\n",
    "\n",
    "    #recebe função de leitura\n",
    "    df = read_df(df)\n",
    "\n",
    "    #verificando se falta alguma coluna\n",
    "    lista_primarias = ['RQI','pressao','pressao_inversa','distancia','permeabilidade','porosidade']\n",
    "    lista_Js = ['J1', 'J2', 'J3', 'J4', 'J5', 'J6', 'J7', 'J8']  \n",
    "    \n",
    "    #verifica se o df possui as keys necessárias\n",
    "    for i in df.keys(): \n",
    "        if lista_primarias.count(i) == 0 and lista_Js.count(i) == 0:\n",
    "            raise KeyError(f'variavel {i} não identificada')                   \n",
    "            \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Calculo dos J's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normaliza_J (df, J, norm=False):\n",
    "    \n",
    "    #recebendo função de checagem\n",
    "    df = checking(df)\n",
    "    \n",
    "    #atribuição        \n",
    "    for j in J: \n",
    "              \n",
    "        if j == 'J1':\n",
    "            J1 = df['RQI']\n",
    "            df['J1'] = J1\n",
    "            normaliza_(df, j, norm)            \n",
    "                    \n",
    "        if j == 'J2':\n",
    "            J2 = df['RQI']*df['pressao']\n",
    "            df['J2'] = J2\n",
    "            normaliza_(df, j, norm)\n",
    "\n",
    "        if j == 'J3':\n",
    "            J3 = df['RQI']*df['pressao_inversa']\n",
    "            df['J3'] = J3\n",
    "            normaliza_(df, j, norm)                                                                                         \n",
    "                            \n",
    "        if j == 'J4':\n",
    "            #para evitar resultados infinitos\n",
    "            aux = np.log(df['distancia']) #calc do log(0 ind, 1 inf)\n",
    "            #mascara\n",
    "            aux[np.isinf(aux)] = 0 \n",
    "            aux[np.isnan(aux)] = 0\n",
    "            aux[aux < 0] = 0\n",
    "\n",
    "            J4 = df['RQI'] * df['pressao'] * aux      \n",
    "            df['J4'] = J4\n",
    "            normaliza_(df, j, norm) \n",
    "\n",
    "        if j == 'J5':\n",
    "\n",
    "            #para evitar resultados infinitos\n",
    "            aux = np.log(df['distancia']) #calc do log(0 ind, 1 inf)\n",
    "            #mascara \n",
    "            aux[np.isinf(aux)] = 0 \n",
    "            aux[np.isnan(aux)] = 0\n",
    "            aux[aux < 0] = 0\n",
    "\n",
    "            J5 = df['RQI'] * df['pressao_inversa'] * aux\n",
    "            df['J5'] = J5\n",
    "            normaliza_(df, j, norm)\n",
    "        \n",
    "        if j == 'J6':\n",
    "\n",
    "            #para evitar resultados infinitos\n",
    "            aux = np.log(df['distancia']) #calc do log(0 ind, 1 inf)\n",
    "            #mascara \n",
    "            aux[np.isinf(aux)] = 0 \n",
    "            aux[np.isnan(aux)] = 0\n",
    "            aux[aux < 0] = 0\n",
    "\n",
    "            J6 = df['permeabilidade'] * df['porosidade'] * aux\n",
    "            df['J6'] = J6\n",
    "            normaliza_(df, j, norm)  \n",
    "\n",
    "    return df\n",
    "\n",
    "def normaliza_(df, col, norm):\n",
    "    if norm == True:   \n",
    "        df[f'{col}_normalizado'] = (df[col] - min(df[col])) / (max(df[col]) - min(df[col]))\n",
    "    #print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Processamento Binning <br> (Bayesian Blocks ou KDE)\n",
    "<br/>\n",
    "A classe binning é responsável por processar os dados em KDE ou Bayesian Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@jit(nopython=True) # Set \"nopython\" mode for best performance, equivalent to @njit\n",
    "class binning():\n",
    "\n",
    "        #método construtor\n",
    "        def __init__(self) -> None:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        #@jit(nopython=True)\n",
    "        def proc_binning(df, J, binning): #recebe o df e o tipo de binning \n",
    "\n",
    "            if binning == 'kde':\n",
    "                \n",
    "                '''Bibliotecas'''\n",
    "                import numpy as np\n",
    "                from numpy import array, linspace\n",
    "                from sklearn.neighbors import KernelDensity\n",
    "                from scipy.misc import electrocardiogram\n",
    "                from scipy.signal import argrelmin, find_peaks\n",
    "                from sklearn.model_selection import GridSearchCV, LeaveOneOut\n",
    "                import scipy.integrate as integrate\n",
    "                              \n",
    "                                     \n",
    "                # método de bandwith para kde\n",
    "     \n",
    "                #encontrar as curvas com bandwidth ideal com validacao cruzada\n",
    "                #BANDWIDTH ORIGINAL: 0.01\n",
    "                ideal_band = 0\n",
    "                for coluna in J:                   \n",
    "                    X = df[f'{coluna}'].values[::].reshape(-1, 1)\n",
    "                \n",
    "                    band = np.linspace(0.01, 0.05)\n",
    "                    grid = GridSearchCV(KernelDensity(), {'bandwidth': band},cv=LeaveOneOut())\n",
    "                    grid.fit(X[:,None].reshape(-1, 1)) \n",
    "                    ideal_band = grid.best_params_\n",
    "                    #print(ideal_band)\n",
    "                    \n",
    "                    #para o J1 simplificado ele retorna {'bandwidth': 0.01}\n",
    "                    #para os outros Js faltou memoria para rodar o codigo. \n",
    "                   \n",
    "                #Calculo KDE   \n",
    "\n",
    "                resultados_kde = {}\n",
    "\n",
    "                for coluna in J:\n",
    "                    \n",
    "                    min_i = 0\n",
    "                    max_i = df[f'{coluna}'].shape[0]#pega a dimensao da coluna \n",
    "                    aux = max_i - min_i\n",
    "                    #print(\"aux\", aux)\n",
    "\n",
    "                    # faz o reshape para encaixar os dados em uma dimensão apropriada para o calculo \n",
    "                    X = df[f'{coluna}'].values[::].reshape(-1, 1)\n",
    "                    #print(\"x\", X)\n",
    "\n",
    "                    # calculo da densidade de probabilidade\n",
    "                    kde = KernelDensity(kernel='gaussian', bandwidth = ideal_band['bandwidth']).fit(X)\n",
    "                    #print(\"kde\", kde)\n",
    "                    \n",
    "                    #retorna os pontos em uma distancia equidistante / os dados precisam está em 2D\n",
    "                    dist = np.linspace([0], [1.0], aux)\n",
    "                    #print(\"dist\", dist)\n",
    "\n",
    "                    #calcula a probabilidade logarítmica de cada amostra sob o modelo                     \n",
    "                    #for i in range(len(dist)):\n",
    "                    log = kde.score_samples(dist.reshape(-1,1))\n",
    "                    #faz uma multiplicação entre os valores de dist e log resolvendo o problema de dimensões diferentes\n",
    "                    #dist.dot(log)\n",
    "                    #print(\"log\", log)                   \n",
    "                    \n",
    "                    # usamos exponencial para deixar log positivo\n",
    "                    #essa integral retorna o valor True ou False (estava retornando valor antes das modificações em dist e log)\n",
    "                    # [:, None] é usado para realizar o broadcast dos dados (operação entre arrays de dimensões diferentes))\n",
    "                    integral = float(np.exp(integrate.trapz(log[:, None], dist[:, None])).all())\n",
    "                    #print(integral)\n",
    "                    \n",
    "                    #if 0.99 <= integral <= 1.01:\n",
    "                    if integral <= 1.01 and integral >= 0.99:\n",
    "                        print(f'A integral : {integral} é adequeada.')\n",
    "                    else:\n",
    "                        print(f'A integral não está no intervalo correto: {integral}')\n",
    "                        raise KeyError(f'A integral precisa estar entre 0.99 e 1.01')\n",
    "\n",
    "\n",
    "                                                           \n",
    "                    '''Calculo das Particoes'''\n",
    "\n",
    "                    # calcula os picos\n",
    "                    peaks = find_peaks(log, height=min(log)) \n",
    "                    #Calcula a minima relativa dos dados\n",
    "                    valleys = argrelmin(log)[0] \n",
    "                    \n",
    "                    #Retorna os indices que classificam o array em ordem crescente\n",
    "                    ord = np.argsort(np.abs(np.diff(log[valleys])))\n",
    "                    #inverte a ordem do array \n",
    "                    ordValleys_J = np.flip(ord)+1 \n",
    "\n",
    "\n",
    "                    #Contrução dos intervalos de classe \n",
    "                    #utiliza o intervalo de 0 a 1 para encontrar os pontos de partição usando a minima relativa\n",
    "                    ov = np.concatenate((np.array([0]), ordValleys_J, np.array([1])))\n",
    "                    classes = {}\n",
    "                    for i in range(1, len(ov)):\n",
    "                        classes[i] = (ov[i-1], ov[i])                    \n",
    "                                      \n",
    "                    #retorno de funcão KDE com dicionario\n",
    "\n",
    "                    resultados_kde[coluna] = {}\n",
    "                    \n",
    "                    resultados_kde[coluna]['dist'] = dist\n",
    "                    resultados_kde[coluna]['log'] = log\n",
    "                    resultados_kde[coluna]['peaks'] = peaks\n",
    "                    resultados_kde[coluna]['valleys'] = valleys\n",
    "                    resultados_kde[coluna]['ordValleys_J'] = ordValleys_J\n",
    "                    resultados_kde[coluna]['classes'] = classes\n",
    "\n",
    "                return resultados_kde   \n",
    "                \n",
    "            \n",
    "            #Calculo Bayesian Blocks\n",
    "\n",
    "            if binning == 'bb':\n",
    "                from astropy.stats import bayesian_blocks\n",
    "\n",
    "                \n",
    "                resultados_bb = {}\n",
    "\n",
    "                for coluna in df.columns:\n",
    "                    if coluna in df.columns[12::]:\n",
    "                        serie = df.query(f\"{coluna} > 0\")[coluna]\n",
    "                        resultados_bb[coluna] = [serie, bayesian_blocks(serie)]\n",
    "\n",
    "            return resultados_bb\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Adição do padding '''\n",
    "\n",
    "def padding(df):\n",
    "\n",
    "    padDF = np.pad(df, (0.01, 0.01)) #para a melhor localização de pontos divisores de classe\n",
    "    return padDF    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle (df, fileout):\n",
    "    import pickle as pkl "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Análise dos Gráficos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficos dos Binnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class graficos:\n",
    "\n",
    "    #método construtor\n",
    "    def __init__(self) -> None:\n",
    "\n",
    "        plt.style.use(\".dados/plot/ccs.mplstyle\")\n",
    "    \n",
    "    def grafico_kde(proc, J):        \n",
    "\n",
    "        #df deve ser aquele resultante do processamento\n",
    "        df = binning.proc_binning(proc, J, binning='kde')\n",
    "\n",
    "        #if padding == True:\n",
    "        #    df = padding(df)\n",
    "\n",
    "        for coluna in df:                        \n",
    "            plt.plot(df[coluna]['dist'], df[coluna]['log'])\n",
    "\n",
    "        for i in J:           \n",
    "            \n",
    "            plt.title(f'Vales dos Kernels | {i} normalizado')\n",
    "            plt.ylabel(\"Densidade\")\n",
    "            plt.xlabel(f'Posição assumida por {i} norm')                      \n",
    "            \n",
    "            plt.plot(df[coluna]['valleys'], df[coluna]['log'][df[coluna]['valleys']], 'o')\n",
    "            plt.plot(df[coluna]['valleys'][df[coluna]['ordValleys_J'][0:4]], df[coluna]['log'][df[coluna]['valleys'][df[coluna]['ordValleys_J'][0:4]]], '*', c='red')\n",
    "            \n",
    "            for j in coluna:\n",
    "                plt.plot(int(coluna['classes'][j]))\n",
    "            plt.show()\n",
    "\n",
    "            for j in df[i]['valleys']:\n",
    "                print(df[i]['valleys'][j])\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "    def grafico_bb(df, bins_bb, J):\n",
    "\n",
    "        df = binning.proc_binning(df, J, binning='bb')\n",
    "\n",
    "        \"\"\"\n",
    "        Essa funcao tem como objetivo criar uma representação gráfica\n",
    "        de uma determinada 'coluna' de um determinado DataFrame (df)\n",
    "        dado o calculo de seus bins/classes baseados no Bayesian Blocks.\n",
    "        Pode-se obter os Bayesian Blocks de uma determinada variável\n",
    "        usando a funcao 'bayesian_blocks' do módulo astropy.stats\n",
    "        \"\"\"\n",
    "                \n",
    "        \n",
    "        labels = {\n",
    "            \"J1_normalizado\" : \"RQI\",\n",
    "            \"J2_normalizado\" : \"RQI * Pressao\",\n",
    "            \"J3_normalizado\" : \"RQI * Pressao Inversa\",\n",
    "            \"J4_normalizado\" : \"RQI * Pressao * ln(distancia)\",\n",
    "            \"J5_normalizado\" : \"RQI * Pressao Inversa * ln(distancia)\", \n",
    "            \"J6_normalizado\" : \"Permeabilidade * Porosidade * ln(distancia)\",\n",
    "            \"J7_normalizado\" : \"ln(Permeabilidade) * ln(Porosidade) * ln(distancia)\"\n",
    "        }\n",
    "        \n",
    "        ax = plt.figure(figsize=(20, 12))\n",
    "        #modificar labels para for\n",
    "        ax = plt.title(f\"Histograma '{J} = {labels[J]}' utilizando blocos bayesianos\", fontsize = 24)\n",
    "        ax = plt.xlabel(\"X\", fontsize = 18)\n",
    "        ax = plt.ylabel(\"Y\", fontsize = 18)\n",
    "        ax = plt.hist(df, bins = bins_bb, color='g')\n",
    "        ax = plt.grid(True)\n",
    "        plt.savefig(f'./dados/Analise de Js/bb/{J}.jpeg', format='jpeg')\n",
    "        plt.show(ax)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Menu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui vamos chamar o pipeline e apresentar opções ao usuário chamando as funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* TRIL - NetZero *******\n",
      "********* Bem Vindo **********\n"
     ]
    }
   ],
   "source": [
    "print(' TRIL - NetZero '.center(30,'*'))\n",
    "print(' Bem Vindo '.center(30,'*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arq = str(print(\"Insira o caminho para o arquivo: \")) #recebemos o csv\n",
    "\n",
    "#valor J1 já normalizado\n",
    "arq = pd.read_csv(\"./dados/Simplificados/J1_reduzido.csv\") \n",
    "#arq = pd.read_csv(\"./dados/variaveis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df com valores NAN\n",
      "df corrigido, não mais possui valores NAN\n"
     ]
    }
   ],
   "source": [
    "#chamando a funçao de preparação\n",
    "arq = read_df(arq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lista_Js = [\\'J1\\', \\'J2\\', \\'J3\\', \\'J4\\', \\'J5\\', \\'J6\\', \\'J7\\', \\'J8\\']  \\n\\nif arq.keys() == lista_Js:\\n    print(\"df com todas as variaveis primarias\")'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"lista_Js = ['J1', 'J2', 'J3', 'J4', 'J5', 'J6', 'J7', 'J8']  \n",
    "\n",
    "if arq.keys() == lista_Js:\n",
    "    print(\"df com todas as variaveis primarias\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df com valores NAN\n",
      "df corrigido, não mais possui valores NAN\n"
     ]
    }
   ],
   "source": [
    "arq = checking(arq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#J = arq.keys().tolist() #pegando as colunas como lista para transformação\\nJ = ['J1', 'J2', 'J3', 'J4', 'J5', 'J6']\\n#J = ['J1'] #JÁ NORMALIZADO\\narq = normaliza_J(arq, J, norm=True)\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#J = arq.keys().tolist() #pegando as colunas como lista para transformação\n",
    "J = ['J1', 'J2', 'J3', 'J4', 'J5', 'J6']\n",
    "#J = ['J1'] #JÁ NORMALIZADO\n",
    "arq = normaliza_J(arq, J, norm=True)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "arq = arq.sample(1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'binning' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m J \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mJ1\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      2\u001b[0m \u001b[39m#J = ['J1', 'J2', 'J3', 'J4', 'J5', 'J6']\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m proc \u001b[39m=\u001b[39m binning\u001b[39m.\u001b[39mproc_binning(arq, J, binning\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mkde\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'binning' is not defined"
     ]
    }
   ],
   "source": [
    "J = ['J1']\n",
    "#J = ['J1', 'J2', 'J3', 'J4', 'J5', 'J6']\n",
    "proc = binning.proc_binning(arq, J, binning='kde')\n",
    "#proc = binning.proc_binning(arq, J, binning='bb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'J1': {'dist': array([[0.00000000e+00],\n",
       "         [6.67111408e-04],\n",
       "         [1.33422282e-03],\n",
       "         ...,\n",
       "         [9.98665777e-01],\n",
       "         [9.99332889e-01],\n",
       "         [1.00000000e+00]]),\n",
       "  'log': array([   1.37022354,    1.36799835,    1.36132279, ..., -307.92102114,\n",
       "         -309.568982  , -311.22139323]),\n",
       "  'peaks': (array([ 237,  295,  344,  427,  573,  711,  765, 1025, 1127], dtype=int64),\n",
       "   {'peak_heights': array([ 1.39742941,  1.14918961,  1.23419014,  0.90167389,  0.5005387 ,\n",
       "           -0.04286341, -0.05395955, -2.03456499, -3.62706877])}),\n",
       "  'valleys': array([  73,  271,  314,  404,  536,  687,  738,  994, 1092], dtype=int64),\n",
       "  'ordValleys_J': array([1, 8, 7, 5, 4, 3, 6, 2], dtype=int64),\n",
       "  'classes': {1: (0, 1),\n",
       "   2: (1, 8),\n",
       "   3: (8, 7),\n",
       "   4: (7, 5),\n",
       "   5: (5, 4),\n",
       "   6: (4, 3),\n",
       "   7: (3, 6),\n",
       "   8: (6, 2),\n",
       "   9: (2, 1)}}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'binning' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m J \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mJ1\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m pl \u001b[39m=\u001b[39m graficos\u001b[39m.\u001b[39;49mgrafico_kde(arq, J)\n",
      "Cell \u001b[1;32mIn[14], line 11\u001b[0m, in \u001b[0;36mgraficos.grafico_kde\u001b[1;34m(proc, J)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgrafico_kde\u001b[39m(proc, J):        \n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m     \u001b[39m#df deve ser aquele resultante do processamento\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     df \u001b[39m=\u001b[39m binning\u001b[39m.\u001b[39mproc_binning(proc, J, binning\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mkde\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m     \u001b[39m#if padding == True:\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[39m#    df = padding(df)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[39mfor\u001b[39;00m coluna \u001b[39min\u001b[39;00m df:                        \n",
      "\u001b[1;31mNameError\u001b[0m: name 'binning' is not defined"
     ]
    }
   ],
   "source": [
    "J = ['J1']\n",
    "pl = graficos.grafico_kde(arq, J) #debug"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''cont = 0\n",
    "for coluna in variaveis_df.columns[12:]:\n",
    "    for valor in variaveis_df[coluna]:\n",
    "        if valor <= 0:\n",
    "            print(coluna, valor)\n",
    "        else:\n",
    "            cont += 1\n",
    "            \n",
    "#verificando se há algum valor nulo ou menor que nulo. Não temos!\n",
    "#verifica se o total é igual ao n de linhas * n de colunas dos J's normalizados\n",
    "cont == variaveis_df.shape[0] * len(variaveis_df.columns[12:])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#gerando gráficos bb\n",
    "resultados = {}\n",
    "\n",
    "for coluna in variaveis_df.columns:\n",
    "    if coluna in variaveis_df.columns[12::]:\n",
    "        serie = variaveis_df.query(f\"{coluna} > 0\")[coluna]\n",
    "        resultados[coluna] = [serie, bayesian_blocks(serie)]\n",
    "        grafico_bb(resultados[coluna][0], resultados[coluna][1], coluna)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2e71ff5d6cfd00d4bc2f505165b5cdad2e2f53d03e64c6e9eb61202e17bc590f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('tril')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "784ac644c6c4bc21869e2a5fc6787b9327a3f2a01dac292daa186c0832dc0a9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
