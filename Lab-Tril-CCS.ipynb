{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab. TRIL - Carbon Capture and Storage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Engenharia Computacional para a Emissão Zero no Setor de Óleo e Gás"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse é um programa que realiza um processo de leitura, processamento e análise de um conjunto de dados de um reservatório, para encontrar classes de injetividade apresentadas com análise de gráfico.\n",
    "\n",
    "Esse projeto é desenvolvido por:<br>\n",
    "* Yhasmim de Souza Tigre - Aluna de Iniciação Científica - UFPB\n",
    "* Prof. Dr. Gustavo C. Oliveira - Professor Orientador - UFPB\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from numba import jit \n",
    "#%matplotlib inline \n",
    "#from matplotlib.pyplot import plotpip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "#Serve para ignorar os 'red warnings' que algumas bibliotecas apontam porque tem novas versoes de implementacao\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Preparação de Files "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essas funções devem receber arquivos csv (em uma formatação pré-estabelecida) que serem lidos, checados e iniciados.<br>\n",
    "Os arquivos precisam:\n",
    "* ter colunas nomeadas (de preferência em ordem alfabética)\n",
    "* não ter valores NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''função de leitura do arquivo csv para df'''\n",
    "\n",
    "def read_df(df):\n",
    "    #limpeza (valores NAN)\n",
    "    if any(df.isna()):\n",
    "        print(\"df com valores NAN\")\n",
    "        #for i in df: #testar sem for\n",
    "        df = df.fillna(0)\n",
    "        print(\"df corrigido, não mais possui valores NAN\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''função de checagem de keys em dataframe '''\n",
    "\n",
    "def checking(df):\n",
    "\n",
    "    #recebe função de leitura\n",
    "    df = read_df(df)\n",
    "\n",
    "    #verificando se falta alguma coluna\n",
    "    lista_primarias = ['RQI','pressao','pressao_inversa','distancia','permeabilidade','porosidade']\n",
    "    lista_Js = ['J1', 'J2', 'J3', 'J4', 'J5', 'J6', 'J7', 'J8']  \n",
    "    \n",
    "    #verifica se o df possui as keys necessárias\n",
    "    for i in df.keys(): \n",
    "        if lista_primarias.count(i) == 0 and lista_Js.count(i) == 0:\n",
    "            raise KeyError(f'variavel {i} não identificada')                   \n",
    "            \n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Calculo dos J's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normaliza_J (df, J, norm=False):\n",
    "    \n",
    "    #recebendo função de checagem\n",
    "    df = checking(df)\n",
    "    \n",
    "    #atribuição        \n",
    "    for j in J: \n",
    "              \n",
    "        if j == 'J1':\n",
    "            J1 = df['RQI']\n",
    "            df['J1'] = J1\n",
    "            normaliza_(df, j, norm)            \n",
    "                    \n",
    "        if j == 'J2':\n",
    "            J2 = df['RQI']*df['pressao']\n",
    "            df['J2'] = J2\n",
    "            normaliza_(df, j, norm)\n",
    "\n",
    "        if j == 'J3':\n",
    "            J3 = df['RQI']*df['pressao_inversa']\n",
    "            df['J3'] = J3\n",
    "            normaliza_(df, j, norm)                                                                                         \n",
    "                            \n",
    "        if j == 'J4':\n",
    "            #para evitar resultados infinitos\n",
    "            aux = np.log(df['distancia']) #calc do log(0 ind, 1 inf)\n",
    "            #mascara\n",
    "            aux[np.isinf(aux)] = 0 \n",
    "            aux[np.isnan(aux)] = 0\n",
    "            aux[aux < 0] = 0\n",
    "\n",
    "            J4 = df['RQI'] * df['pressao'] * aux      \n",
    "            df['J4'] = J4\n",
    "            normaliza_(df, j, norm) \n",
    "\n",
    "        if j == 'J5':\n",
    "\n",
    "            #para evitar resultados infinitos\n",
    "            aux = np.log(df['distancia']) #calc do log(0 ind, 1 inf)\n",
    "            #mascara \n",
    "            aux[np.isinf(aux)] = 0 \n",
    "            aux[np.isnan(aux)] = 0\n",
    "            aux[aux < 0] = 0\n",
    "\n",
    "            J5 = df['RQI'] * df['pressao_inversa'] * aux\n",
    "            df['J5'] = J5\n",
    "            normaliza_(df, j, norm)\n",
    "        \n",
    "        if j == 'J6':\n",
    "\n",
    "            #para evitar resultados infinitos\n",
    "            aux = np.log(df['distancia']) #calc do log(0 ind, 1 inf)\n",
    "            #mascara \n",
    "            aux[np.isinf(aux)] = 0 \n",
    "            aux[np.isnan(aux)] = 0\n",
    "            aux[aux < 0] = 0\n",
    "\n",
    "            J6 = df['permeabilidade'] * df['porosidade'] * aux\n",
    "            df['J6'] = J6\n",
    "            normaliza_(df, j, norm)  \n",
    "\n",
    "    return df\n",
    "\n",
    "def normaliza_(df, col, norm):\n",
    "    if norm == True:   \n",
    "        df[f'{col}_normalizado'] = (df[col] - min(df[col])) / (max(df[col]) - min(df[col]))\n",
    "    #print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Processamento Binning <br> (Bayesian Blocks ou KDE)\n",
    "<br/>\n",
    "A classe binning é responsável por processar os dados em KDE ou Bayesian Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class binning():\n",
    "\n",
    "        #método construtor\n",
    "        def __init__(self) -> None:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        #@jit(nopython=True)\n",
    "        def proc_binning(df, J, binning): #recebe o df e o tipo de binning \n",
    "\n",
    "            if binning == 'kde':\n",
    "                \n",
    "                '''Bibliotecas'''\n",
    "                import numpy as np\n",
    "                from numpy import array, linspace\n",
    "                from sklearn.neighbors import KernelDensity\n",
    "                from scipy.misc import electrocardiogram\n",
    "                from scipy.signal import argrelmin, find_peaks\n",
    "                from sklearn.model_selection import GridSearchCV, LeaveOneOut\n",
    "                import scipy.integrate as integrate\n",
    "                              \n",
    "                                     \n",
    "                # método de bandwith para kde\n",
    "     \n",
    "                #encontrar as curvas com bandwidth ideal com validacao cruzada\n",
    "                #BANDWIDTH ORIGINAL: 0.01\n",
    "                ideal_band = 0\n",
    "                for coluna in J:                   \n",
    "                    X = df[f'{coluna}'].values[::].reshape(-1, 1)\n",
    "                \n",
    "                    band = np.linspace(0.01, 0.05)\n",
    "                    grid = GridSearchCV(KernelDensity(), {'bandwidth': band},cv=LeaveOneOut())\n",
    "                    grid.fit(X[:,None].reshape(-1, 1)) \n",
    "                    ideal_band = grid.best_params_\n",
    "                    #print(ideal_band)\n",
    "                    \n",
    "                    #para o J1 simplificado ele retorna {'bandwidth': 0.01}\n",
    "                    #para os outros Js faltou memoria para rodar o codigo. \n",
    "                   \n",
    "                #Calculo KDE   \n",
    "\n",
    "                resultados_kde = {}\n",
    "\n",
    "                for coluna in J:\n",
    "                    \n",
    "                    min_i = 0\n",
    "                    max_i = df[f'{coluna}'].shape[0]#pega a dimensao da coluna \n",
    "                    aux = max_i - min_i\n",
    "                    #print(\"aux\", aux)\n",
    "\n",
    "                    # faz o reshape para encaixar os dados em uma dimensão apropriada para o calculo \n",
    "                    X = df[f'{coluna}'].values[::].reshape(-1, 1)\n",
    "                    #print(\"x\", X)\n",
    "\n",
    "                    # calculo da densidade de probabilidade\n",
    "                    kde = KernelDensity(kernel='gaussian', bandwidth = ideal_band['bandwidth']).fit(X)\n",
    "                    #print(\"kde\", kde)\n",
    "                    \n",
    "                    #retorna os pontos em uma distancia equidistante / os dados precisam está em 2D\n",
    "                    dist = np.linspace([0], [1.0], aux)\n",
    "                    #print(\"dist\", dist)\n",
    "\n",
    "                    #calcula a probabilidade logarítmica de cada amostra sob o modelo                     \n",
    "                    #for i in range(len(dist)):\n",
    "                    log = kde.score_samples(dist.reshape(-1,1))\n",
    "                    #faz uma multiplicação entre os valores de dist e log resolvendo o problema de dimensões diferentes\n",
    "                    #dist.dot(log)\n",
    "                    #print(\"log\", log)                   \n",
    "                    \n",
    "                    # usamos exponencial para deixar log positivo\n",
    "                    #essa integral retorna o valor True ou False (estava retornando valor antes das modificações em dist e log)\n",
    "                    # [:, None] é usado para realizar o broadcast dos dados (operação entre arrays de dimensões diferentes))\n",
    "                    integral = float(np.exp(integrate.trapz(log[:, None], dist[:, None])).all())\n",
    "                    #outra forma de calcular a integral\n",
    "                    #integral = integrate.trapezoid(y=np.exp(log), x=None, dx=dist, axis=-1)\n",
    "                    print(integral)\n",
    "                    \n",
    "                    #if 0.99 <= integral <= 1.01:\n",
    "                    if integral <= 1.01 and integral >= 0.99:\n",
    "                        print(f'A integral : {integral} é adequeada.')\n",
    "                    else:\n",
    "                        print(f'A integral não está no intervalo correto: {integral}')\n",
    "                        raise KeyError(f'A integral precisa estar entre 0.99 e 1.01')\n",
    "\n",
    "\n",
    "                                                           \n",
    "                    '''Calculo das Particoes'''\n",
    "\n",
    "                    # calcula os picos\n",
    "                    peaks = find_peaks(log, height=min(log)) \n",
    "                    #Calcula a minima relativa dos dados\n",
    "                    valleys = argrelmin(log)[0] \n",
    "                    \n",
    "                    #Retorna os indices que classificam o array em ordem crescente\n",
    "                    ord = np.argsort(np.abs(np.diff(log[valleys])))\n",
    "                    #inverte a ordem do array \n",
    "                    ordValleys_J = np.flip(ord)+1 \n",
    "\n",
    "\n",
    "                    #Contrução dos intervalos de classe \n",
    "                    #utiliza o intervalo de 0 a 1 para encontrar os pontos de partição usando a minima relativa\n",
    "                    ov = np.concatenate((np.array([0]), ordValleys_J, np.array([1])))\n",
    "                    classes = {}\n",
    "                    for i in range(1, len(ov)):\n",
    "                        classes[i] = (ov[i-1], ov[i])                    \n",
    "                                      \n",
    "                    #retorno de funcão KDE com dicionario\n",
    "                    \n",
    "                    resultados_kde[coluna]['dist'] = dist\n",
    "                    resultados_kde[coluna]['log'] = log\n",
    "                    resultados_kde[coluna]['peaks'] = peaks\n",
    "                    resultados_kde[coluna]['valleys'] = valleys\n",
    "                    resultados_kde[coluna]['ordValleys_J'] = ordValleys_J\n",
    "                    resultados_kde[coluna]['classes'] = classes\n",
    "\n",
    "                return resultados_kde   \n",
    "                \n",
    "            \n",
    "            #Calculo Bayesian Blocks\n",
    "\n",
    "            if binning == 'bb':\n",
    "                from astropy.stats import bayesian_blocks\n",
    "\n",
    "                \n",
    "                resultados_bb = {}\n",
    "\n",
    "                for coluna in df.columns:\n",
    "                    if coluna in df.columns[12::]:\n",
    "                        serie = df.query(f\"{coluna} > 0\")[coluna]\n",
    "                        resultados_bb[coluna] = [serie, bayesian_blocks(serie)]\n",
    "\n",
    "            return resultados_bb\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n                    log = kde.score_samples(dist.reshape(-1,1)) \\n                    print(kde.score_samples)\\n                    print(log)\\n                    \\n                    \\n                    # usamos exponencial para deixar log positivo\\n                    integral = np.exp(integrate.trapz(log, dist))\\n\\n                    print(integral)\\n                    \\n                    #if 0.99 <= integral <= 1.01:\\n                    if integral <= 1.01 and integral >= 0.99:\\n                        print(f'A integral : {integral} é adequeada.')\\n                    else:\\n                        print(f'A integral não está no intervalo correto: {integral}')\\n                        raise KeyError(f'A integral precisa estar entre 0.99 e 1.01')\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "                    log = kde.score_samples(dist.reshape(-1,1)) \n",
    "                    print(kde.score_samples)\n",
    "                    print(log)\n",
    "                    \n",
    "                    \n",
    "                    # usamos exponencial para deixar log positivo\n",
    "                    integral = np.exp(integrate.trapz(log, dist))\n",
    "\n",
    "                    print(integral)\n",
    "                    \n",
    "                    #if 0.99 <= integral <= 1.01:\n",
    "                    if integral <= 1.01 and integral >= 0.99:\n",
    "                        print(f'A integral : {integral} é adequeada.')\n",
    "                    else:\n",
    "                        print(f'A integral não está no intervalo correto: {integral}')\n",
    "                        raise KeyError(f'A integral precisa estar entre 0.99 e 1.01')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Adição do padding '''\n",
    "\n",
    "def padding(df):\n",
    "\n",
    "    padDF = np.pad(df, (0.01, 0.01)) #para a melhor localização de pontos divisores de classe\n",
    "    return padDF\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle (df, fileout):\n",
    "    import pickle as pkl \n",
    "\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Análise dos Gráficos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficos dos Binnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class graficos:\n",
    "\n",
    "    #método construtor\n",
    "    def __init__(self) -> None:\n",
    "\n",
    "        plt.style.use(\".dados/plot/ccs.mplstyle\")\n",
    "    \n",
    "    def grafico_kde(proc, J):        \n",
    "\n",
    "        #df deve ser aquele resultante do processamento\n",
    "        df = binning.proc_binning(proc, J, binning='kde')\n",
    "  \n",
    "        plt.plot(df['dist'], df['log'])\n",
    "\n",
    "        for i in J:           \n",
    "            \n",
    "            plt.title(f'Vales dos Kernels | {i} normalizado')\n",
    "            plt.ylabel(\"Densidade\")\n",
    "            plt.xlabel(f'Posição assumida por {i} norm')\n",
    "                      \n",
    "            \n",
    "            plt.plot(df[i]['valleys'], df[i]['log'][df[i]['valleys']], 'o')\n",
    "            plt.plot(df[i]['valleys'][df[i]['ordValleys_J'][0:4]], df[i]['log'][df[i]['valleys'][df[i]['ordValleys_J'][0:4]]], '*', c='red')\n",
    "            plt.plot(df[i]['classes'])\n",
    "            plt.show()\n",
    "\n",
    "            for j in df[i]['valleys']:\n",
    "                print(df[i]['valleys'][j])\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "    def grafico_bb(df, bins_bb, J):\n",
    "\n",
    "        df = binning.proc_binning(df, J, binning='bb')\n",
    "\n",
    "        \"\"\"\n",
    "        Essa funcao tem como objetivo criar uma representação gráfica\n",
    "        de uma determinada 'coluna' de um determinado DataFrame (df)\n",
    "        dado o calculo de seus bins/classes baseados no Bayesian Blocks.\n",
    "        Pode-se obter os Bayesian Blocks de uma determinada variável\n",
    "        usando a funcao 'bayesian_blocks' do módulo astropy.stats\n",
    "        \"\"\"\n",
    "                \n",
    "        \n",
    "        labels = {\n",
    "            \"J1_normalizado\" : \"RQI\",\n",
    "            \"J2_normalizado\" : \"RQI * Pressao\",\n",
    "            \"J3_normalizado\" : \"RQI * Pressao Inversa\",\n",
    "            \"J4_normalizado\" : \"RQI * Pressao * ln(distancia)\",\n",
    "            \"J5_normalizado\" : \"RQI * Pressao Inversa * ln(distancia)\", \n",
    "            \"J6_normalizado\" : \"Permeabilidade * Porosidade * ln(distancia)\",\n",
    "            \"J7_normalizado\" : \"ln(Permeabilidade) * ln(Porosidade) * ln(distancia)\"\n",
    "        }\n",
    "        \n",
    "        ax = plt.figure(figsize=(20, 12))\n",
    "        #modificar labels para for\n",
    "        ax = plt.title(f\"Histograma '{J} = {labels[J]}' utilizando blocos bayesianos\", fontsize = 24)\n",
    "        ax = plt.xlabel(\"X\", fontsize = 18)\n",
    "        ax = plt.ylabel(\"Y\", fontsize = 18)\n",
    "        ax = plt.hist(df, bins = bins_bb, color='g')\n",
    "        ax = plt.grid(True)\n",
    "        plt.savefig(f'./dados/Analise de Js/bb/{J}.jpeg', format='jpeg')\n",
    "        plt.show(ax)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Menu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui vamos chamar o pipeline e apresentar opções ao usuário chamando as funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* TRIL - NetZero *******\n",
      "********* Bem Vindo **********\n"
     ]
    }
   ],
   "source": [
    "print(' TRIL - NetZero '.center(30,'*'))\n",
    "print(' Bem Vindo '.center(30,'*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arq = str(print(\"Insira o caminho para o arquivo: \")) #recebemos o csv\n",
    "\n",
    "#valor J1 já normalizado\n",
    "arq = pd.read_csv(\"./dados/Simplificados/J1_reduzido.csv\") \n",
    "#arq = pd.read_csv(\"./dados/variaveis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df com valores NAN\n",
      "df corrigido, não mais possui valores NAN\n"
     ]
    }
   ],
   "source": [
    "#chamando a funçao de preparação\n",
    "arq = read_df(arq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Shapes must match', (1,), (8,))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m lista_Js \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mJ1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mJ2\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mJ3\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mJ4\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mJ5\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mJ6\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mJ7\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mJ8\u001b[39m\u001b[39m'\u001b[39m]  \n\u001b[1;32m----> 3\u001b[0m \u001b[39mif\u001b[39;00m a\u001b[39m.\u001b[39;49mkeys() \u001b[39m==\u001b[39;49m lista_Js:\n\u001b[0;32m      4\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdf com todas as variaveis primarias\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\pandas\\core\\ops\\common.py:72\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[0;32m     70\u001b[0m other \u001b[39m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 72\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, other)\n",
      "File \u001b[1;32mc:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\pandas\\core\\arraylike.py:42\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39m@unpack_zerodim_and_defer\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m__eq__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__eq__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[1;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cmp_method(other, operator\u001b[39m.\u001b[39;49meq)\n",
      "File \u001b[1;32mc:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\pandas\\core\\indexes\\base.py:7023\u001b[0m, in \u001b[0;36mIndex._cmp_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   7020\u001b[0m \u001b[39melif\u001b[39;00m is_object_dtype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ABCMultiIndex):\n\u001b[0;32m   7021\u001b[0m     \u001b[39m# don't pass MultiIndex\u001b[39;00m\n\u001b[0;32m   7022\u001b[0m     \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(\u001b[39mall\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 7023\u001b[0m         result \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mcomp_method_OBJECT_ARRAY(op, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_values, other)\n\u001b[0;32m   7025\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   7026\u001b[0m     \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(\u001b[39mall\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:72\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[1;34m(op, x, y)\u001b[0m\n\u001b[0;32m     69\u001b[0m         y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39m_values\n\u001b[0;32m     71\u001b[0m     \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m y\u001b[39m.\u001b[39mshape:\n\u001b[1;32m---> 72\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mShapes must match\u001b[39m\u001b[39m\"\u001b[39m, x\u001b[39m.\u001b[39mshape, y\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     73\u001b[0m     result \u001b[39m=\u001b[39m libops\u001b[39m.\u001b[39mvec_compare(x\u001b[39m.\u001b[39mravel(), y\u001b[39m.\u001b[39mravel(), op)\n\u001b[0;32m     74\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: ('Shapes must match', (1,), (8,))"
     ]
    }
   ],
   "source": [
    "lista_Js = ['J1', 'J2', 'J3', 'J4', 'J5', 'J6', 'J7', 'J8']  \n",
    "\n",
    "if arq.keys() == lista_Js:\n",
    "    print(\"df com todas as variaveis primarias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df com valores NAN\n",
      "df corrigido, não mais possui valores NAN\n"
     ]
    }
   ],
   "source": [
    "arq = checking(arq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df com valores NAN\n",
      "df corrigido, não mais possui valores NAN\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'RQI'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'RQI'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m J \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mJ1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mJ2\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mJ3\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mJ4\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mJ5\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mJ6\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m \u001b[39m#J = ['J1'] #JÁ NORMALIZADO\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m arq \u001b[39m=\u001b[39m normaliza_J(arq, J, norm\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m, in \u001b[0;36mnormaliza_J\u001b[1;34m(df, J, norm)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m J: \n\u001b[0;32m      9\u001b[0m     \u001b[39mif\u001b[39;00m j \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mJ1\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m---> 10\u001b[0m         J1 \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mRQI\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[0;32m     11\u001b[0m         df[\u001b[39m'\u001b[39m\u001b[39mJ1\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m J1\n\u001b[0;32m     12\u001b[0m         normaliza_(df, j, norm)            \n",
      "File \u001b[1;32mc:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\pandas\\core\\frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3803\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3804\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3805\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3806\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3807\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3806\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3807\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3810\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'RQI'"
     ]
    }
   ],
   "source": [
    "#J = arq.keys().tolist() #pegando as colunas como lista para transformação\n",
    "J = ['J1', 'J2', 'J3', 'J4', 'J5', 'J6']\n",
    "#J = ['J1'] #JÁ NORMALIZADO\n",
    "arq = normaliza_J(arq, J, norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "arq = arq.sample(1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m J \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mJ1\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      2\u001b[0m \u001b[39m#J = ['J1', 'J2', 'J3', 'J4', 'J5', 'J6']\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m proc \u001b[39m=\u001b[39m binning\u001b[39m.\u001b[39;49mproc_binning(arq, J, binning\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mkde\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[7], line 14\u001b[0m, in \u001b[0;36mbinning.proc_binning\u001b[1;34m(df, J, binning)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mimport\u001b[39;00m array, linspace\n\u001b[1;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mneighbors\u001b[39;00m \u001b[39mimport\u001b[39;00m KernelDensity\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmisc\u001b[39;00m \u001b[39mimport\u001b[39;00m electrocardiogram\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msignal\u001b[39;00m \u001b[39mimport\u001b[39;00m argrelmin, find_peaks\n",
      "File \u001b[1;32mc:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\sklearn\\__init__.py:82\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _distributor_init  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m __check_build  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m clone\n\u001b[0;32m     83\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_show_versions\u001b[39;00m \u001b[39mimport\u001b[39;00m show_versions\n\u001b[0;32m     85\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[0;32m     86\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcalibration\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     87\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcluster\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mshow_versions\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    129\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\sklearn\\base.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_config\u001b[39;00m \u001b[39mimport\u001b[39;00m get_config\n\u001b[1;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m _IS_32BIT\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_set_output\u001b[39;00m \u001b[39mimport\u001b[39;00m _SetOutputMixin\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_tags\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     _DEFAULT_TAGS,\n\u001b[0;32m     21\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\sklearn\\utils\\__init__.py:21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmurmurhash\u001b[39;00m \u001b[39mimport\u001b[39;00m murmurhash3_32\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mclass_weight\u001b[39;00m \u001b[39mimport\u001b[39;00m compute_class_weight, compute_sample_weight\n\u001b[1;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _joblib\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m DataConversionWarning\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdeprecation\u001b[39;00m \u001b[39mimport\u001b[39;00m deprecated\n",
      "File \u001b[1;32mc:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\sklearn\\utils\\_joblib.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m _warnings\u001b[39m.\u001b[39msimplefilter(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[39m# joblib imports may raise DeprecationWarning on certain Python\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39m# versions\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjoblib\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjoblib\u001b[39;00m \u001b[39mimport\u001b[39;00m logger\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mjoblib\u001b[39;00m \u001b[39mimport\u001b[39;00m dump, load\n",
      "File \u001b[1;32mc:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\joblib\\__init__.py:120\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mnumpy_pickle\u001b[39;00m \u001b[39mimport\u001b[39;00m load\n\u001b[0;32m    119\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcompressor\u001b[39;00m \u001b[39mimport\u001b[39;00m register_compressor\n\u001b[1;32m--> 120\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mparallel\u001b[39;00m \u001b[39mimport\u001b[39;00m Parallel\n\u001b[0;32m    121\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mparallel\u001b[39;00m \u001b[39mimport\u001b[39;00m delayed\n\u001b[0;32m    122\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mparallel\u001b[39;00m \u001b[39mimport\u001b[39;00m cpu_count\n",
      "File \u001b[1;32mc:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\joblib\\parallel.py:26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlogger\u001b[39;00m \u001b[39mimport\u001b[39;00m Logger, short_format_time\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdisk\u001b[39;00m \u001b[39mimport\u001b[39;00m memstr_to_bytes\n\u001b[1;32m---> 26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_parallel_backends\u001b[39;00m \u001b[39mimport\u001b[39;00m (FallbackToBackend, MultiprocessingBackend,\n\u001b[0;32m     27\u001b[0m                                  ThreadingBackend, SequentialBackend,\n\u001b[0;32m     28\u001b[0m                                  LokyBackend)\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mexternals\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcloudpickle\u001b[39;00m \u001b[39mimport\u001b[39;00m dumps, loads\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m eval_expr\n",
      "File \u001b[1;32mc:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\joblib\\_parallel_backends.py:17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_multiprocessing_helpers\u001b[39;00m \u001b[39mimport\u001b[39;00m mp\n\u001b[0;32m     16\u001b[0m \u001b[39mif\u001b[39;00m mp \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 17\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpool\u001b[39;00m \u001b[39mimport\u001b[39;00m MemmappingPool\n\u001b[0;32m     18\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mmultiprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpool\u001b[39;00m \u001b[39mimport\u001b[39;00m ThreadPool\n\u001b[0;32m     19\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mexecutor\u001b[39;00m \u001b[39mimport\u001b[39;00m get_memmapping_executor\n",
      "File \u001b[1;32mc:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\joblib\\pool.py:31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpickle\u001b[39;00m \u001b[39mimport\u001b[39;00m HIGHEST_PROTOCOL\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mio\u001b[39;00m \u001b[39mimport\u001b[39;00m BytesIO\n\u001b[1;32m---> 31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_memmapping_reducer\u001b[39;00m \u001b[39mimport\u001b[39;00m get_memmapping_reducers\n\u001b[0;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_memmapping_reducer\u001b[39;00m \u001b[39mimport\u001b[39;00m TemporaryResourcesManager\n\u001b[0;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_multiprocessing_helpers\u001b[39;00m \u001b[39mimport\u001b[39;00m mp, assert_spawning\n",
      "File \u001b[1;32mc:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\joblib\\_memmapping_reducer.py:37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbackports\u001b[39;00m \u001b[39mimport\u001b[39;00m make_memmap\n\u001b[0;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdisk\u001b[39;00m \u001b[39mimport\u001b[39;00m delete_folder\n\u001b[1;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mexternals\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloky\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m \u001b[39mimport\u001b[39;00m resource_tracker\n\u001b[0;32m     39\u001b[0m \u001b[39m# Some system have a ramdisk mounted by default, we can use it instead of /tmp\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[39m# as the default folder to dump big arrays to share with subprocesses.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m SYSTEM_SHARED_MEM_FS \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/dev/shm\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\joblib\\externals\\loky\\__init__.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcontext\u001b[39;00m \u001b[39mimport\u001b[39;00m cpu_count\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreduction\u001b[39;00m \u001b[39mimport\u001b[39;00m set_loky_pickler\n\u001b[1;32m---> 20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mreusable_executor\u001b[39;00m \u001b[39mimport\u001b[39;00m get_reusable_executor\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcloudpickle_wrapper\u001b[39;00m \u001b[39mimport\u001b[39;00m wrap_non_picklable_objects\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mprocess_executor\u001b[39;00m \u001b[39mimport\u001b[39;00m BrokenProcessPool, ProcessPoolExecutor\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:846\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:941\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1040\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "J = ['J1']\n",
    "#J = ['J1', 'J2', 'J3', 'J4', 'J5', 'J6']\n",
    "proc = binning.proc_binning(arq, J, binning='kde')\n",
    "#proc = binning.proc_binning(arq, J, binning='bb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'log': array([   1.33875738,    1.33830512,    1.33694834, ..., -216.49720953,\n",
       "        -217.11705576, -217.73780651]),\n",
       " 'peaks': (array([ 524,  663,  754, 1206, 1284, 1588, 1688, 2031, 2278, 2500, 2638],\n",
       "        dtype=int64),\n",
       "  {'peak_heights': array([ 1.40214603,  1.21323607,  1.16051964,  0.50818918,  0.50314996,\n",
       "          -0.09382513, -0.12178598, -1.19328021, -2.06334845, -4.42200619,\n",
       "          -4.42312746])}),\n",
       " 'valleys': array([ 163,  601,  726, 1133, 1252, 1534, 1648, 2013, 2235, 2441, 2569],\n",
       "       dtype=int64),\n",
       " 'ordValleys_J': array([ 1,  9,  7,  8,  3,  5, 10,  2,  6,  4], dtype=int64),\n",
       " 'classes': {1: (0, 1),\n",
       "  2: (1, 9),\n",
       "  3: (9, 7),\n",
       "  4: (7, 8),\n",
       "  5: (8, 3),\n",
       "  6: (3, 5),\n",
       "  7: (5, 10),\n",
       "  8: (10, 2),\n",
       "  9: (2, 6),\n",
       "  10: (6, 4),\n",
       "  11: (4, 1)}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "A integral : 1.0 é adequeada.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'J1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m J \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mJ1\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m pl \u001b[39m=\u001b[39m graficos\u001b[39m.\u001b[39;49mgrafico_kde(arq, J)\n",
      "Cell \u001b[1;32mIn[59], line 11\u001b[0m, in \u001b[0;36mgraficos.grafico_kde\u001b[1;34m(proc, J)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgrafico_kde\u001b[39m(proc, J):        \n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m     \u001b[39m#df deve ser aquele resultante do processamento\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     df \u001b[39m=\u001b[39m binning\u001b[39m.\u001b[39;49mproc_binning(proc, J, binning\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mkde\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     13\u001b[0m     plt\u001b[39m.\u001b[39mplot(df[\u001b[39m'\u001b[39m\u001b[39mdist\u001b[39m\u001b[39m'\u001b[39m], df[\u001b[39m'\u001b[39m\u001b[39mlog\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     15\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m J:           \n",
      "Cell \u001b[1;32mIn[58], line 109\u001b[0m, in \u001b[0;36mbinning.proc_binning\u001b[1;34m(df, J, binning)\u001b[0m\n\u001b[0;32m    105\u001b[0m     classes[i] \u001b[39m=\u001b[39m (ov[i\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], ov[i])                    \n\u001b[0;32m    107\u001b[0m \u001b[39m#retorno de funcão KDE com dicionario\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m resultados_kde[coluna][\u001b[39m'\u001b[39m\u001b[39mdist\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m dist\n\u001b[0;32m    110\u001b[0m resultados_kde[coluna][\u001b[39m'\u001b[39m\u001b[39mlog\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m log\n\u001b[0;32m    111\u001b[0m resultados_kde[coluna][\u001b[39m'\u001b[39m\u001b[39mpeaks\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m peaks\n",
      "\u001b[1;31mKeyError\u001b[0m: 'J1'"
     ]
    }
   ],
   "source": [
    "J = ['J1']\n",
    "pl = graficos.grafico_kde(arq, J) #debug"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = 0\n",
    "for coluna in variaveis_df.columns[12:]:\n",
    "    for valor in variaveis_df[coluna]:\n",
    "        if valor <= 0:\n",
    "            print(coluna, valor)\n",
    "        else:\n",
    "            cont += 1\n",
    "            \n",
    "#verificando se há algum valor nulo ou menor que nulo. Não temos!\n",
    "#verifica se o total é igual ao n de linhas * n de colunas dos J's normalizados\n",
    "cont == variaveis_df.shape[0] * len(variaveis_df.columns[12:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gerando gráficos bb\n",
    "resultados = {}\n",
    "\n",
    "for coluna in variaveis_df.columns:\n",
    "    if coluna in variaveis_df.columns[12::]:\n",
    "        serie = variaveis_df.query(f\"{coluna} > 0\")[coluna]\n",
    "        resultados[coluna] = [serie, bayesian_blocks(serie)]\n",
    "        grafico_bb(resultados[coluna][0], resultados[coluna][1], coluna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2e71ff5d6cfd00d4bc2f505165b5cdad2e2f53d03e64c6e9eb61202e17bc590f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('tril')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "784ac644c6c4bc21869e2a5fc6787b9327a3f2a01dac292daa186c0832dc0a9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
