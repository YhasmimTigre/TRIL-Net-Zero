{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab. TRIL - Net Zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Engenharia Computacional para a Emissão Zero no Setor de Óleo e Gás"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse é um programa que realiza um processo de leitura, processamento e análise de um conjunto de dados de um reservatório, para encontrar classes de injetividade apresentadas com análise de gráfico.\n",
    "\n",
    "As funcões a serem desenvolvidas são:\n",
    "* read_df (recebe inputs csv ou df)\n",
    "* calc_j (de J para J normalizado)\n",
    "* proc (KDE e Bayesian Blocks)\n",
    "* class (encontrar os pontos e classes)\n",
    "* graf_j (analise dos graficos )\n",
    "\n",
    "Esse projeto é desenvolvido por:<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "#Serve para ignorar os 'red warnings' que algumas bibliotecas apontam porque tem novas versoes de implementacao\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Preparação de Files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essas funções devem receber arquivos csv (em uma formatação pré-estabelecida) que serem lidos, checados e iniciados.<br>\n",
    "Os arquivos precisam:\n",
    "* ter colunas nomeadas (de preferência em ordem alfabética)\n",
    "* não ter valores NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''função de leitura do arquivo csv para df'''\n",
    "\n",
    "def read_df(arq):\n",
    "    #leitura pelo pandas\n",
    "    df = arq           \n",
    "    #limpeza (valores NAN)\n",
    "    if any(df.isna()):\n",
    "        print(\"df com valores NAN\")\n",
    "        for i in df:\n",
    "            df = df.fillna(0)\n",
    "        print(\"df corrigido, não mais possui valores NAN\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''função de checagem de keys em dataframe '''\n",
    "\n",
    "def checking(df):\n",
    "\n",
    "    #recebe função de leitura\n",
    "    df = read_df(df)\n",
    "\n",
    "    #verificando se falta alguma coluna\n",
    "    lista_primarias = ['RQI','pressao','pressao_inversa','distancia','permeabilidade','porosidade']\n",
    "    lista_Js = ['J1', 'J2', 'J3', 'J4', 'J5', 'J6', 'J7', 'J8']  \n",
    "    \n",
    "    #verifica se o df possui as keys necessárias\n",
    "    for i in df.keys(): \n",
    "        if lista_primarias.count(i) == 0 and lista_Js.count(i) == 0:\n",
    "            raise KeyError(f'variavel {i} não identificada')                      \n",
    "            \n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Calculo dos J's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normaliza_J (df, J, norm=False):\n",
    "    \n",
    "    #recebendo função de checagem\n",
    "    df = checking(df)\n",
    "    \n",
    "    #atribuição        \n",
    "    for j in J: \n",
    "              \n",
    "        if j == 'J1':\n",
    "            J1 = df['RQI']\n",
    "            df['J1'] = J1\n",
    "            normaliza_(df, j, norm)            \n",
    "                    \n",
    "        if j == 'J2':\n",
    "            J2 = df['RQI']*df['pressao']\n",
    "            df['J2'] = J2\n",
    "            normaliza_(df, j, norm)\n",
    "\n",
    "        if j == 'J3':\n",
    "            J3 = df['RQI']*df['pressao_inversa']\n",
    "            df['J3'] = J3\n",
    "            normaliza_(df, j, norm)                                                                                         \n",
    "                            \n",
    "        if j == 'J4':\n",
    "            #para evitar resultados infinitos\n",
    "            aux = np.log(df['distancia']) #calc do log(0 ind, 1 inf)\n",
    "            #mascara\n",
    "            aux[np.isinf(aux)] = 0 \n",
    "            aux[np.isnan(aux)] = 0\n",
    "            aux[aux < 0] = 0\n",
    "\n",
    "            J4 = df['RQI'] * df['pressao'] * aux      \n",
    "            df['J4'] = J4\n",
    "            normaliza_(df, j, norm)\n",
    "\n",
    "        if j == 'J5':\n",
    "\n",
    "            #para evitar resultados infinitos\n",
    "            aux = np.log(df['distancia']) #calc do log(0 ind, 1 inf)\n",
    "            #mascara \n",
    "            aux[np.isinf(aux)] = 0 \n",
    "            aux[np.isnan(aux)] = 0\n",
    "            aux[aux < 0] = 0\n",
    "\n",
    "            J5 = df['RQI'] * df['pressao_inversa'] * aux\n",
    "            df['J5'] = J5\n",
    "            normaliza_(df, j, norm)\n",
    "        \n",
    "        if j == 'J6':\n",
    "\n",
    "            #para evitar resultados infinitos\n",
    "            aux = np.log(df['distancia']) #calc do log(0 ind, 1 inf)\n",
    "            #mascara \n",
    "            aux[np.isinf(aux)] = 0 \n",
    "            aux[np.isnan(aux)] = 0\n",
    "            aux[aux < 0] = 0\n",
    "\n",
    "            J6 = df['permeabilidade'] * df['porosidade'] * aux\n",
    "            df['J6'] = J6\n",
    "            normaliza_(df, j, norm)  \n",
    "\n",
    "    return df\n",
    "\n",
    "def normaliza_(df, col, norm):\n",
    "    if norm == True:   \n",
    "        df[f'{col}_normalizado'] = (df[col] - min(df[col])) / (max(df[col]) - min(df[col]))\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Processamento Binning <br> (Bayesian Blocks ou KDE)\n",
    "<br/>\n",
    "A classe binning é responsável por processar os dados em KDE ou Bayesian Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class binning():\n",
    "\n",
    "        #método construtor\n",
    "        def __init__(self) -> None:\n",
    "            pass\n",
    "    \n",
    "        def proc_binning(df, J, binning): #recebe o df e o tipo de binning \n",
    "\n",
    "            if binning == 'kde':\n",
    "                \n",
    "                '''Bibliotecas'''\n",
    "                import numpy as np\n",
    "                from numpy import array, linspace\n",
    "                from sklearn.neighbors import KernelDensity\n",
    "                from scipy.misc import electrocardiogram\n",
    "                from scipy.signal import argrelmin, find_peaks\n",
    "                from sklearn.model_selection import GridSearchCV, LeaveOneOut\n",
    "                import scipy.integrate as integrate\n",
    "                \n",
    "                from keras.models import Sequential\n",
    "                from keras.layers import Dense\n",
    "                from keras.layers import Dropout\n",
    "                from keras.wrappers.scikit_learn import KerasClassifier\n",
    "                from keras.constraints import maxnorm\n",
    "                from keras.utils import np_utils              \n",
    "                \n",
    "\n",
    "                #recebendo J normalizados\n",
    "                '''\n",
    "                #Calculo Bandwith \n",
    "                for coluna in J:\n",
    "                    J = df[f'{coluna}'].values\n",
    "                    bandwidth = 1.06*np.std(J) * ((-1/2) ** len(J))\n",
    "                    print(\"bandwidth: \", bandwidth)\n",
    "                '''           \n",
    "    \n",
    "                #encontrar as curvas com bandwidth ideal com validacao cruzada\n",
    "                #BANDWIDTH ORIGINAL: 0.01\n",
    "\n",
    "\n",
    "                \n",
    "                for coluna in J:                   \n",
    "                    X = df[f'{coluna}'].values[::].reshape(-1, 1)\n",
    "                    bandwidths = np.linspace(0.01, 0.05)\n",
    "                    grid = GridSearchCV(KernelDensity(), {'bandwidth': bandwidths},cv=LeaveOneOut())\n",
    "                    grid.fit(X[:, None]) \n",
    "                    ideal_band = grid.best_params_\n",
    "                    print(ideal_band)\n",
    "                         \n",
    "                        \n",
    "                #Calculo KDE   \n",
    "\n",
    "                for coluna in J:\n",
    "                    min_i = 0\n",
    "                    max_i = df[f'{coluna}'].shape[0]#pega a dimensao da coluna \n",
    "                    aux = max_i - min_i\n",
    "\n",
    "                    kde= KernelDensity(bandwidth = 0.01).fit(X)\n",
    "                    dist = np.linspace(0, 1.0, aux) #retorna os pontos em uma distancia equidistante\n",
    "                    log = kde.score_samples(dist.reshape(-1,1)) #calcula a probabilidade logarítmica de cada amostra sob o modelo\n",
    "\n",
    "                    '''Calculo das Particoes'''\n",
    "\n",
    "                    peaks = find_peaks(log, height=min(log)) # calcula os picos\n",
    "                    valleys = argrelmin(log)[0] #Calcula a minima relativa dos dados\n",
    "                    \n",
    "                    ord = np.argsort(np.abs(np.diff(log[valleys])))#Retorna os indices que classificam o array\n",
    "                    ordValleys_J = np.flip(ord)+1 #inverte a ordem do array\n",
    "\n",
    "                #Encontrando Classes \n",
    "                '''\n",
    "                for coluna in J:\n",
    "\n",
    "                    X = sorted(df[f'{coluna}'])                           \n",
    "                    minimo = min(X)\n",
    "                    classes = [minimo]\n",
    "                    for i in range(len(peaks)):\n",
    "                        classes.append(X(peaks[i]))                        \n",
    "                    classes.append(max(X))\n",
    "                '''\n",
    "                #Calculo da Integral KDE\n",
    "\n",
    "                #calculo da integral sobre o score_samples\n",
    "                integral = integrate.trapezoid(np.exp(log),dist) # usamos exponencial para deixar log positivo\n",
    "                if 0.99 < integral < 1.01:\n",
    "                    print(f'A integral : {integral} é adequeada.')\n",
    "                else:\n",
    "                    print(f'A integral não está no intervalo correto: {integral}')\n",
    "                    raise KeyError(f'A integral precisa estar entre 0.99 e 1.01')\n",
    "                \n",
    "                resultados_kde = []\n",
    "                resultados_kde [log, peaks, valleys, ordValleys_J]\n",
    "                return resultados_kde    \n",
    "           \n",
    "\n",
    "            #Calculo Bayesian Blocks\n",
    "\n",
    "            if binning == 'bb':\n",
    "                from astropy.stats import bayesian_blocks\n",
    "                \n",
    "                resultados = {}\n",
    "\n",
    "                for coluna in df.columns:\n",
    "                    if coluna in df.columns[12::]:\n",
    "                        serie = df.query(f\"{coluna} > 0\")[coluna]\n",
    "                        resultados[coluna] = [serie, bayesian_blocks(serie)]\n",
    "\n",
    "            return resultados   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Adição do padding '''\n",
    "\n",
    "def padding(df):\n",
    "\n",
    "    padDF = np.pad(df, (0.01, 0.01)) #para a melhor localização de pontos divisores de classe\n",
    "    return padDF\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle (df, fileout):\n",
    "    import pickle as pkl  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graficos dos Binnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Análise dos Gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class graficos:\n",
    "    \"\"\"\n",
    "    def grafico_kde(df, coluna):\n",
    "        \n",
    "        #df deve ser aquele resultante do processamento\n",
    "\n",
    "        plt.figure(figsize=(20, 12))\n",
    "        #adaptar para cada J\n",
    "        plt.title(\"Vales dos Kernels | J2 normalizado\", fontsize=22)\n",
    "        plt.ylabel(\"Densidade\", fontsize=16)\n",
    "        plt.xlabel(\"Posição assumida por J2 norm\", fontsize=16)\n",
    "        plt.grid(True)\n",
    "        #in a for\n",
    "        plt.plot(x)\n",
    "        plt.plot(valleys, x[valleys], 'o')\n",
    "        plt.plot(valleys[ordValleys_J[0:4]], x[valleys[ordValleys_J[0:4]]], '*', c='green')\n",
    "        plt.show()\n",
    "\n",
    "        for i in df:\n",
    "            print(valleys)\n",
    "\n",
    "    def grafico_bb(df, bins_bb, coluna):\n",
    "\n",
    "        #modificar labels para for\n",
    "        labels = {\n",
    "            \"J1_normalizado\" : \"RQI\",\n",
    "            \"J2_normalizado\" : \"RQI * Pressao\",\n",
    "            \"J3_normalizado\" : \"RQI * Pressao Inversa\",\n",
    "            \"J4_normalizado\" : \"RQI * Pressao * ln(distancia)\",\n",
    "            \"J5_normalizado\" : \"RQI * Pressao Inversa * ln(distancia)\", \n",
    "            \"J6_normalizado\" : \"Permeabilidade * Porosidade * ln(distancia)\",\n",
    "            \"J7_normalizado\" : \"ln(Permeabilidade) * ln(Porosidade) * ln(distancia)\"\n",
    "        }\n",
    "        \n",
    "        ax = plt.figure(figsize=(20, 12))\n",
    "        ax = plt.title(f\"Histograma '{coluna} = {labels[coluna]}' utilizando blocos bayesianos\", fontsize = 24)\n",
    "        ax = plt.xlabel(\"X\", fontsize = 18)\n",
    "        ax = plt.ylabel(\"Y\", fontsize = 18)\n",
    "        ax = plt.hist(df, bins = bins_bb, color='g')\n",
    "        ax = plt.grid(True)\n",
    "        plt.savefig(f'./dados/Analise de Js/{coluna}.jpeg', format='jpeg')\n",
    "        plt.show(ax)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Menu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui vamos chamar o pipeline e apresentar opções ao usuário chamando as funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' TRIL - NetZero '.center(30,'*'))\n",
    "print(' Bem Vindo '.center(30,'*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arq = str(print(\"Insira o caminho para o arquivo: \")) #recebemos o csv\n",
    "\n",
    "arq = pd.read_csv(\"./dados/Simplificados/J1_reduzido.csv\") #valor J1\n",
    "#arq = pd.read_csv(\"./dados/variaveis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df com valores NAN\n",
      "df corrigido, não mais possui valores NAN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>J1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.42121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.41414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.45838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.39962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.34914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3321</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3322</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3323</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3324</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3325</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3326 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           J1\n",
       "0     0.42121\n",
       "1     0.41414\n",
       "2     0.45838\n",
       "3     0.39962\n",
       "4     0.34914\n",
       "...       ...\n",
       "3321  0.00000\n",
       "3322  0.00000\n",
       "3323  0.00000\n",
       "3324  0.00000\n",
       "3325  0.00000\n",
       "\n",
       "[3326 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#chamando a funçao de preparação\n",
    "read_df(arq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df com valores NAN\n",
      "df corrigido, não mais possui valores NAN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>J1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.42121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.41414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.45838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.39962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.34914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3321</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3322</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3323</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3324</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3325</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3326 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           J1\n",
       "0     0.42121\n",
       "1     0.41414\n",
       "2     0.45838\n",
       "3     0.39962\n",
       "4     0.34914\n",
       "...       ...\n",
       "3321  0.00000\n",
       "3322  0.00000\n",
       "3323  0.00000\n",
       "3324  0.00000\n",
       "3325  0.00000\n",
       "\n",
       "[3326 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checking(arq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df com valores NAN\n",
      "df corrigido, não mais possui valores NAN\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'RQI'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'RQI'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#J = arq.keys().tolist() #pegando as colunas como lista para transformação\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#J = ['J1', 'J2', 'J3', 'J4', 'J5', 'J6']\u001b[39;00m\n\u001b[0;32m      3\u001b[0m J \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mJ1\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m----> 4\u001b[0m arq \u001b[39m=\u001b[39m normaliza_J(arq, J, norm\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "Cell \u001b[1;32mIn[6], line 10\u001b[0m, in \u001b[0;36mnormaliza_J\u001b[1;34m(df, J, norm)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m J: \n\u001b[0;32m      9\u001b[0m     \u001b[39mif\u001b[39;00m j \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mJ1\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m---> 10\u001b[0m         J1 \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mRQI\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[0;32m     11\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m df[\u001b[39m'\u001b[39m\u001b[39mRQI\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m     12\u001b[0m              df[\u001b[39m'\u001b[39m\u001b[39mJ1\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m J1\n",
      "File \u001b[1;32mc:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\pandas\\core\\frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3803\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3804\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3805\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3806\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3807\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3806\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3807\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3810\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'RQI'"
     ]
    }
   ],
   "source": [
    "#J = arq.keys().tolist() #pegando as colunas como lista para transformação\n",
    "#J = ['J1', 'J2', 'J3', 'J4', 'J5', 'J6']\n",
    "J = ['J1']\n",
    "arq = normaliza_J(arq, J, norm=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 166300 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n166300 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 684, in _fit_and_score\n    estimator.fit(X_train, **fit_params)\n  File \"c:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\sklearn\\neighbors\\_kde.py\", line 225, in fit\n    X = self._validate_data(X, order=\"C\", dtype=DTYPE)\n  File \"c:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\sklearn\\base.py\", line 535, in _validate_data\n    X = check_array(X, input_name=\"X\", **check_params)\n  File \"c:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 913, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. KernelDensity expected <= 2.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m J \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mJ1\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      2\u001b[0m \u001b[39m#J = ['J1', 'J2', 'J3', 'J4', 'J5', 'J6']\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m proc \u001b[39m=\u001b[39m binning\u001b[39m.\u001b[39;49mproc_binning(arq, J, binning\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mkde\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      4\u001b[0m \u001b[39m#proc = binning.proc_binning(arq, J, binning='bb')\u001b[39;00m\n\u001b[0;32m      5\u001b[0m proc\n",
      "Cell \u001b[1;32mIn[12], line 46\u001b[0m, in \u001b[0;36mbinning.proc_binning\u001b[1;34m(df, J, binning)\u001b[0m\n\u001b[0;32m     44\u001b[0m bandwidths \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinspace(\u001b[39m0.01\u001b[39m, \u001b[39m0.05\u001b[39m)\n\u001b[0;32m     45\u001b[0m grid \u001b[39m=\u001b[39m GridSearchCV(KernelDensity(), {\u001b[39m'\u001b[39m\u001b[39mbandwidth\u001b[39m\u001b[39m'\u001b[39m: bandwidths},cv\u001b[39m=\u001b[39mLeaveOneOut())\n\u001b[1;32m---> 46\u001b[0m grid\u001b[39m.\u001b[39;49mfit(X[:, \u001b[39mNone\u001b[39;49;00m]) \n\u001b[0;32m     47\u001b[0m ideal_band \u001b[39m=\u001b[39m grid\u001b[39m.\u001b[39mbest_params_\n\u001b[0;32m     48\u001b[0m \u001b[39mprint\u001b[39m(ideal_band)\n",
      "File \u001b[1;32mc:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\sklearn\\model_selection\\_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    869\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    871\u001b[0m     )\n\u001b[0;32m    873\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 875\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    877\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    879\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1389\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1387\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1388\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1389\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\sklearn\\model_selection\\_search.py:852\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    845\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m!=\u001b[39m n_candidates \u001b[39m*\u001b[39m n_splits:\n\u001b[0;32m    846\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    847\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcv.split and cv.get_n_splits returned \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    848\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minconsistent results. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    849\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msplits, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_splits, \u001b[39mlen\u001b[39m(out) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m n_candidates)\n\u001b[0;32m    850\u001b[0m     )\n\u001b[1;32m--> 852\u001b[0m _warn_or_raise_about_fit_failures(out, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_score)\n\u001b[0;32m    854\u001b[0m \u001b[39m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    855\u001b[0m \u001b[39m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[39m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m    857\u001b[0m \u001b[39m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39mif\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscoring):\n",
      "File \u001b[1;32mc:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[39mif\u001b[39;00m num_failed_fits \u001b[39m==\u001b[39m num_fits:\n\u001b[0;32m    361\u001b[0m     all_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAll the \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt is very likely that your model is misconfigured.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou can try to debug the error by setting error_score=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n\u001b[1;32m--> 367\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    369\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     some_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[0;32m    371\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mnum_failed_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed out of a total of \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe score on these train-test partitions for these parameters\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    377\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 166300 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n166300 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 684, in _fit_and_score\n    estimator.fit(X_train, **fit_params)\n  File \"c:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\sklearn\\neighbors\\_kde.py\", line 225, in fit\n    X = self._validate_data(X, order=\"C\", dtype=DTYPE)\n  File \"c:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\sklearn\\base.py\", line 535, in _validate_data\n    X = check_array(X, input_name=\"X\", **check_params)\n  File \"c:\\Users\\Yhasmim\\miniconda3\\envs\\tril\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 913, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. KernelDensity expected <= 2.\n"
     ]
    }
   ],
   "source": [
    "J = ['J1']\n",
    "#J = ['J1', 'J2', 'J3', 'J4', 'J5', 'J6']\n",
    "proc = binning.proc_binning(arq, J, binning='kde')\n",
    "#proc = binning.proc_binning(arq, J, binning='bb')\n",
    "proc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### opções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#o df precisa passar pelo calculo dos J's para prosseguir\n",
    "#o usuário deve ter a opção de escolher quais calculos fazer? sim, quais J' normalizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opcao = 0\n",
    "while opcao != 3:\n",
    "    print('''   Qual tipo de processamento voce deseja executar?\n",
    "            digite [1] para Bayesian Blocks\n",
    "            digite [2] para KDE\n",
    "            digite [3] para ambos\n",
    "            digite [4] para voltar ao passo anterior ''')\n",
    "    opcao = int(input())\n",
    "\n",
    "    #if opcao == 1:\n",
    "        # função BB\n",
    "        #print(\"Processo de Binning por Bayesian Block completado\")\n",
    "        #print(imagens dos graficos de bayesian blocks para cada J normalizado)\n",
    "        #o programa pode salvar essas imagens diretamente em um arquivo\n",
    "\n",
    "    #elif opcao == 2:\n",
    "        #função KDE\n",
    "        #print(\"Processo de Binning por KDE completado\")\n",
    "        #print(imagens dos graficos de KDE para cada J normalizado)\n",
    "        #o programa pode salvar essas imagens diretamente em um arquivo\n",
    "\n",
    "    #elif opcao == 3:\n",
    "        # função BB\n",
    "        #função KDE\n",
    "    \n",
    "    #elif opcao == 4:\n",
    "        #break #volta do começo\n",
    "\n",
    "    #else:\n",
    "        #print(\"Esse e um caractere invalido. Deseja recomeçar? Digite [0]\")\n",
    "        #opcao = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Digite se deseja realizar a classificacao: s/n\")\n",
    "resposta = input()\n",
    "\n",
    "#if resposta == 's':\n",
    "    #retornar funcao de classificacao\n",
    "\n",
    "#if resposta == 'n':\n",
    "    #print(\"Fim do Programa\")\n",
    "#else: \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidth = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa função deve receber arquivos csv (em uma formatação pré-estabelecida) que serem lidos, checados e iniciados.<br>\n",
    "Os arquivos precisam:\n",
    "* ter colunas nomeadas (de preferência em ordem alfabética)\n",
    "* não ter valores NAN\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_df (arquivo):\n",
    "    #leitura pelo pandas\n",
    "    df = pd.read.csv(arquivo, index=False)\n",
    "        \n",
    "    #limpeza (valores NAN)\n",
    "    if df.isna() == True:\n",
    "        print(\"df com NAN \", df)\n",
    "        df = df.fillna(0)\n",
    "\n",
    "    print(df.head())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Calculo dos J's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Processamento Binning <br> (Bayesian Blocks ou KDE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa classificação só acontece por meio do processamento do KDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Análise dos Gráficos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Menu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui vamos chamar o pipeline e apresentar opções ao usuário chamando as funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' TRIL - NetZero '.center(30,'*'))\n",
    "print(' Bem Vindo '.center(30,'*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arq = str(print(\"Insira o caminho para o arquivo: \")) #recebemos o csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chamando a funçao de preparação\n",
    "prep_df(arq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### opções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#o df precisa passar pelo calculo dos J's para prosseguir\n",
    "#o usuário deve ter a opção de escolher quais calculos fazer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opcao = 0\n",
    "while opcao != 3:\n",
    "    print('''   Qual tipo de processamento voce deseja executar?\n",
    "            digite [1] para Bayesian Blocks\n",
    "            digite [2] para KDE\n",
    "            digite [3] para ambos\n",
    "            digite [4] para voltar ao passo anterior ''')\n",
    "    opcao = int(input())\n",
    "\n",
    "    #if opcao == 1:\n",
    "        # função BB\n",
    "        #print(\"Processo de Binning por Bayesian Block completado\")\n",
    "        #print(imagens dos graficos de bayesian blocks para cada J normalizado)\n",
    "        #o programa pode salvar essas imagens diretamente em um arquivo\n",
    "\n",
    "    #elif opcao == 2:\n",
    "        #função KDE\n",
    "        #print(\"Processo de Binning por KDE completado\")\n",
    "        #print(imagens dos graficos de KDE para cada J normalizado)\n",
    "        #o programa pode salvar essas imagens diretamente em um arquivo\n",
    "\n",
    "    #elif opcao == 3:\n",
    "        # função BB\n",
    "        #função KDE\n",
    "    \n",
    "    #elif opcao == 4:\n",
    "        #break #volta do começo\n",
    "\n",
    "    #else:\n",
    "        #print(\"Esse e um caractere invalido. Deseja recomeçar? Digite [0]\")\n",
    "        #opcao = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Digite se deseja realizar a classificacao: s/n\")\n",
    "resposta = input()\n",
    "\n",
    "#if resposta == 's':\n",
    "    #retornar funcao de classificacao\n",
    "\n",
    "#if resposta == 'n':\n",
    "    #print(\"Fim do Programa\")\n",
    "#else:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2e71ff5d6cfd00d4bc2f505165b5cdad2e2f53d03e64c6e9eb61202e17bc590f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('tril')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "784ac644c6c4bc21869e2a5fc6787b9327a3f2a01dac292daa186c0832dc0a9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
